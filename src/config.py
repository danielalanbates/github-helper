"""Configuration management for dogood."""

import os
import subprocess
from pathlib import Path
from dotenv import load_dotenv

PROJECT_ROOT = Path(__file__).parent.parent
load_dotenv(PROJECT_ROOT / ".env")


def _get_gh_token() -> str:
    """Get GitHub token from env or gh CLI."""
    token = os.getenv("GITHUB_TOKEN", "")
    if token:
        return token
    try:
        result = subprocess.run(
            ["gh", "auth", "token"], capture_output=True, text=True, timeout=5
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except Exception:
        pass
    return ""


# --- GitHub ---
GITHUB_TOKEN = _get_gh_token()
GITHUB_USERNAME = os.getenv("GITHUB_USERNAME", "danielalanbates")
GITHUB_GRAPHQL_URL = "https://api.github.com/graphql"

# --- Paths ---
DATA_DIR = PROJECT_ROOT / "data"
DB_PATH = DATA_DIR / "github_helper.db"  # Legacy name kept for backward compatibility
WORK_DIR = Path(os.getenv("WORK_DIR", "/tmp/dogood-workdir"))
SEED_DATA_PATH = PROJECT_ROOT / "awesome-for-beginners" / "data.json"

# --- Rate Limiting ---
SEARCH_DELAY_SECONDS = 2.5  # ~24 searches/min (safe under 30/min limit)

# --- Scoring Weights ---
WEIGHT_POPULARITY = 0.20
WEIGHT_SOCIAL_IMPACT = 0.30
WEIGHT_NEED = 0.25
WEIGHT_KINDNESS = 0.25

# --- Social Impact Topics ---
SOCIAL_TOPICS = {
    "nonprofit", "charity", "social-impact", "tech-for-good",
    "accessibility", "a11y", "education", "edtech", "health",
    "healthcare", "climate", "sustainability", "environment",
    "open-data", "civic-tech", "humanitarian", "disaster-relief",
    "food-security", "clean-water", "poverty", "human-rights",
    "mental-health", "assistive-technology",
}

# --- Bounty labels (top priority, always use opus) ---
BOUNTY_LABELS = {
    "bounty", "reward", "paid", "cash", "ðŸ’°", "ðŸ’µ",
    "has bounty", "has-bounty", "bounty-posted",
}

# --- Labels reserved for human beginners (we skip these) ---
BEGINNER_LABELS = {
    "good first issue", "good-first-issue", "beginner",
    "easy", "first-timers-only", "starter", "low-hanging-fruit",
    "up-for-grabs",
}

# --- Ethics Filter ---
# Repos/topics we refuse to contribute to. We only help build things that
# are wholesome and align with Christian values.
BLOCKED_TOPICS = {
    "nsfw", "adult", "porn", "pornography", "sex", "erotic", "hentai",
    "xxx", "nudity", "onlyfans", "cam", "escort", "fetish", "strip",
    "gambling", "casino", "betting", "slot-machine",
    "drugs", "drug-market", "darknet-market",
    "weapon", "exploit-kit", "malware", "ransomware", "spyware",
    "occult", "satanism", "witchcraft",
}
BLOCKED_DESCRIPTION_KEYWORDS = {
    "pornograph", "adult content", "nsfw", "erotic", "sex toy",
    "gambling platform", "casino", "betting platform",
    "drug market", "darknet", "exploit kit", "malware",
}

# --- Anti-AI Policy Detection ---
# Keywords in CONTRIBUTING.md / README.md that indicate AI contributions are banned.
ANTI_AI_KEYWORDS = {
    "no ai", "no llm", "ai-generated", "ban ai", "no bots",
    "ai contributions not accepted", "ai-free", "no machine",
    "ai-generated contributions", "do not use ai", "don't use ai",
    "generated by ai", "written by ai", "no copilot",
    "no chatgpt", "no claude", "llm-generated",
}

# --- Manually Blocked Orgs/Repos ---
# Repos or orgs that silently closed/locked our PRs (anti-AI policy not documented).
BLOCKED_ORGS = {
    "mrdoob",  # three.js â€” closes and locks AI PRs without comment
    "ChatGPTNextWeb",  # NextChat â€” silently closes all PRs
}
BLOCKED_REPOS = {
    "LibreTranslate/LibreTranslate",  # silently closes PRs
}

# --- Supported Languages ---
# We only fix bugs in repos whose primary language is one of these.
SUPPORTED_LANGUAGES = {
    "Python", "JavaScript", "TypeScript", "Shell", "Bash",
    "YAML", "SQL", "HTML", "CSS", "Nunjucks", "EJS", "Astro",
    "Vue", "Svelte", "SCSS", "Less", "Markdown", "Lua",
}

# File extensions we can work with (mapped from SUPPORTED_LANGUAGES)
SUPPORTED_EXTENSIONS = {
    ".py", ".js", ".jsx", ".ts", ".tsx", ".mjs", ".cjs",
    ".sh", ".bash", ".zsh",
    ".yaml", ".yml", ".sql", ".lua",
    ".html", ".htm", ".css", ".scss", ".less",
    ".vue", ".svelte", ".astro", ".ejs", ".njk",
    ".md", ".mdx", ".json", ".toml",
}

# File extensions that signal the bug is NOT in our wheelhouse
UNSUPPORTED_EXTENSIONS = {
    ".rs", ".go", ".java", ".kt", ".scala", ".clj",
    ".c", ".cpp", ".cc", ".cxx", ".h", ".hpp",
    ".cs", ".rb", ".php", ".swift", ".m", ".mm",
    ".r", ".R", ".jl", ".hs", ".erl", ".ex", ".exs",
    ".dart", ".pl", ".pm",
}

# --- CLA/DCO Detection ---
# Only catch repos that EXPLICITLY REQUIRE signing a CLA to contribute.
# We want enforcement language ("must sign", "required to sign", "please sign")
# near CLA mentions â€” NOT informational mentions like "we have a CLA".

# Definite enforcement phrases (substring match, no context needed)
CLA_KEYWORDS = {
    "cla-assistant",              # CLA bot that blocks PRs
    "cla assistant",              # Same bot, different formatting
    "sign the cla",               # Direct instruction to sign
    "sign a cla",                 # Direct instruction to sign
    "sign our cla",               # Direct instruction to sign
    "must sign the contributor",  # "must sign the contributor license agreement"
    "required to sign a contributor", # Enforcement language
    "generative ai agreement",   # AI-specific contributor agreement
    "ai contribution agreement",  # AI-specific contributor agreement
    "ai policy agreement",        # AI policy before contributing
}

# Contextual regex: enforcement language near CLA/contributor agreement
# These catch phrases like "you must sign a CLA before submitting"
CLA_CONTEXTUAL_PATTERNS = [
    # "must/need to/required to/have to/please sign [a/the/our] CLA"
    r'(?:must|need to|required to|have to|please)\s+sign\s+(?:a\s+|the\s+|our\s+)?(?:cla\b|contributor\s+(?:license\s+)?agreement)',
    # "CLA must/needs to/is required to be signed"
    r'\bcla\b\s+(?:must|needs? to|is required to)\s+be\s+signed',
    # "signing (?:a|the|our) CLA is required/mandatory/necessary"
    r'signing\s+(?:a\s+|the\s+|our\s+)?(?:cla\b|contributor\s+(?:license\s+)?agreement)\s+is\s+(?:required|mandatory|necessary)',
    # "submit/accept/complete (?:a|the|our) contributor license agreement"
    r'(?:submit|accept|complete)\s+(?:a\s+|the\s+|our\s+)?contributor\s+license\s+agreement',
    # "PRs require a signed CLA" / "contributions require signing a CLA"
    r'(?:pull requests?|prs?|contributions?)\s+(?:\w+\s+){0,3}require\s+(?:\w+\s+){0,2}(?:sign|cla\b|contributor\s+(?:license\s+)?agreement)',
]

DCO_KEYWORDS = {
    "developer certificate of origin",
    "signed-off-by",
}

# Workflow files that enforce CLA/DCO (these actively block PRs)
CLA_WORKFLOW_FILES = [
    ".github/workflows/cla.yml",
    ".github/workflows/cla.yaml",
]
DCO_WORKFLOW_FILES = [
    ".github/workflows/dco.yml",
    ".github/workflows/dco.yaml",
]

# Org-level CLA blocklist â€” these orgs require CLAs across all/most repos.
# Populated from confirmed blacklist data (repos that already hit CLA detection).
# Per-repo CONTRIBUTING.md scan still catches any we miss here.
CLA_ORGS = {
    "microsoft", "google", "apache", "dotnet", "tensorflow",
    "facebookresearch", "hashicorp", "Shopify", "Azure", "googleapis",
    "salesforce", "angular", "google-deepmind", "facebookarchive",
    "GoogleChrome", "google-research", "adobe", "grafana",
    "googlecolab", "google-gemini", "googlesamples", "googleworkspace",
    "GoogleCloudPlatform", "eclipse", "SAP", "chef",
    "TransformerOptimus",
    "n8n-io", "Significant-Gravitas", "rabbitmq",
}

# Organizations where we HAVE signed the CLA
SIGNED_CLA_ORGS = {
    "facebook", "meta", "pytorch",  # Meta CLA signed (pytorch uses Meta CLA)
}

# --- Multi-Agent Factory ---
MAX_CONCURRENT_AGENTS = int(os.getenv("MAX_CONCURRENT_AGENTS", "4"))  # 4 agents at sonnet-low
AGENT_CLAIM_TTL_MINUTES = 120
MIN_STARS_DEFAULT = 1000  # lowered from 10000
FEEDBACK_POLL_INTERVAL_SECONDS = 300
HOSTILE_SENTIMENT_THRESHOLD = 0.7
MAX_OPUS_PER_ISSUE = 10  # max opus attempts per individual issue before capping at sonnet

# --- Model Tiers (Daniel Tier System) ---
# Edit tiers.json in the project root to change tiers live â€” no restart needed.
# The factory hot-reloads this file on each agent spawn.
TIERS_FILE = PROJECT_ROOT / "tiers.json"
_tiers_cache: dict = {"mtime": 0.0, "tiers": []}


def load_model_tiers() -> list[dict]:
    """Load model tiers from tiers.json, hot-reloading when file changes."""
    import json as _json
    try:
        mtime = TIERS_FILE.stat().st_mtime
        if mtime != _tiers_cache["mtime"]:
            _tiers_cache["tiers"] = _json.loads(TIERS_FILE.read_text())
            _tiers_cache["mtime"] = mtime
        return _tiers_cache["tiers"]
    except Exception:
        pass
    # Fallback if file missing/corrupt
    return [
        {"tier": 1, "model": "claude-sonnet-4-6", "effort": "low", "label": "sonnet-low"},
        {"tier": 2, "model": "claude-sonnet-4-6", "effort": "high", "label": "sonnet-high"},
        {"tier": 3, "model": "claude-opus-4-6", "effort": "high", "label": "opus-high"},
    ]


# Initial load â€” but consumers should call load_model_tiers() for fresh data
MODEL_TIERS = load_model_tiers()

# --- Log file ---
LOG_FILE = PROJECT_ROOT / "Claude Agent - dogood.md"
